{"metadata":{"kernelspec":{"display_name":"CoMA","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Dataset**","metadata":{"id":"EKWorLTmnWje"}},{"cell_type":"code","source":"!gdown 1JJjMiNieTz7xYs6UeVqd02M3DW4fnEfU\n!unzip cvpr2016_flowers.zip","metadata":{"id":"JuQm2mr-nWAJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef load_captions(captions_folder, image_folder):\n    captions = {}\n    image_files = os.listdir(image_folder)\n    for image_file in image_files:\n        image_name = image_file.split('.')[0]\n        caption_file = os.path.join(captions_folder, image_name + \".txt\")\n        with open(caption_file, \"r\") as f:\n            caption = f.readlines()[0].strip()\n        if image_name not in captions:\n                captions[image_name] = caption\n    return captions","metadata":{"id":"JXbAVSehnJSk"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"captions_folder = \"./cvpr2016_flowers/captions\"\nimage_folder = \"./cvpr2016_flowers/images\"\n\ncaptions = load_captions(captions_folder, image_folder)","metadata":{"id":"IdZoPj1rnJSk"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"captions['image_01313']","metadata":{"id":"vs3JngWtnJSk","outputId":"e37546ed-7ee3-4a0f-f655-3d743b736aea"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Caption Encoder**","metadata":{"id":"2Wkwb5djnbmB"}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nbert_model = SentenceTransformer(\"all-mpnet-base-v2\").to(device)\n\ndef encode_captions(captions):\n    encoded_captions = {}\n    for image_name in captions.keys():\n        caption = captions[image_name]\n        encoded_captions[image_name] = {\n            'embed': torch.tensor(bert_model.encode(caption)),\n            'text': caption\n        }\n    return encoded_captions","metadata":{"id":"ReQ7CNBZnJSl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_captions = encode_captions(captions)","metadata":{"id":"cxlnGDOfnJSl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_captions['image_01313']","metadata":{"id":"CZF2UsLwnJSl","outputId":"c119ae9c-82dc-4442-ce82-a82abc9fd4a2"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##**Preprocess**\n","metadata":{"id":"onYxBlIwnhPv"}},{"cell_type":"code","source":"from PIL import Image\nfrom torch.utils.data import Dataset\n\nclass FlowerDataset(Dataset):\n    def __init__(self, img_dir, captions, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n\n        # Load captions\n        self.captions = captions\n\n        self.img_names = list(self.captions.keys())\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, idx):\n        img_name = self.img_names[idx]\n        img_path = os.path.join(self.img_dir, img_name+\".jpg\")\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        encoded_caption = self.captions[img_name]['embed']\n        caption = self.captions[img_name]['text']\n\n        return {\n            'image': image,\n            'embed_caption': encoded_caption,\n            'text': caption\n        }","metadata":{"id":"ttNkBXxUnJSl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\nIMG_SIZE = 128\n\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\nds = FlowerDataset(\n    img_dir=\"./cvpr2016_flowers/images\",\n    captions=encoded_captions,\n    transform=transform\n)","metadata":{"id":"Smfa8iE7nJSl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_grid(img):\n  npimg = img.numpy()\n  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n  plt.show()","metadata":{"id":"HWtpAkulnJSl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(ds)","metadata":{"id":"FovgKLOvnJSl","outputId":"8bc9b75c-0828-468e-d3f3-52021f4607d1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_grid(next(iter(ds))['image'])","metadata":{"id":"xGXg6Oe9nJSl","outputId":"bfa764ab-9fda-4cb0-d27a-5d73a01c38c9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nBATCH_SIZE = 1024\ndataloader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"id":"jsxLZXzNnJSl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_sample = next(iter(dataloader))","metadata":{"id":"dvWFWdt4nJSl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_sample['image'].shape","metadata":{"id":"XAjkfjfanJSl","outputId":"e9f7b176-70bb-41b7-bd97-afa0d11a6df8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\n\nshow_grid(torchvision.utils.make_grid(batch_sample['image'], normalize=True))","metadata":{"id":"kX5GKRAnnJSl"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Model**","metadata":{"id":"5a_V6jydn0At"}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Generator(nn.Module):\n\n    def __init__(self, noise_size, feature_size, num_channels, embedding_size, reduced_dim_size):\n        super(Generator, self).__init__()\n        self.reduced_dim_size = reduced_dim_size\n\n        #768-->256\n        self.textEncoder = nn.Sequential(\n            nn.Linear(in_features = embedding_size, out_features = reduced_dim_size),\n            nn.BatchNorm1d(num_features = reduced_dim_size),\n            nn.LeakyReLU(negative_slope = 0.2, inplace = True)\n        )\n\n        self.upsamplingBlock = nn.Sequential(\n            #256+100 --> 1024\n            nn.ConvTranspose2d(noise_size + reduced_dim_size, feature_size * 8, 4, 1, 0, bias = False),\n            nn.BatchNorm2d(feature_size * 8),\n            nn.LeakyReLU(negative_slope = 0.2, inplace = True),\n\n            # 1024 --> 512\n            nn.ConvTranspose2d(feature_size * 8, feature_size * 4, 4, 2, 1, bias = False),\n            nn.BatchNorm2d(feature_size * 4),\n            nn.ReLU(True),\n\n            # 512 --> 256\n            nn.ConvTranspose2d(feature_size * 4, feature_size * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(feature_size * 2),\n            nn.ReLU(True),\n\n            # 256 --> 128\n            nn.ConvTranspose2d(feature_size * 2, feature_size, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(feature_size),\n            nn.ReLU(True),\n\n            # 128 --> 128\n            nn.ConvTranspose2d(feature_size, feature_size, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(feature_size),\n            nn.ReLU(True),\n\n            # 128 --> 3\n            nn.ConvTranspose2d(feature_size, num_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n\n        )\n\n    def forward(self, noise, text_embeddings):\n        encoded_text = self.textEncoder(text_embeddings)\n        concat_input = torch.cat([noise, encoded_text], dim = 1).unsqueeze(2).unsqueeze(2)\n        output = self.upsamplingBlock(concat_input)\n        return output","metadata":{"id":"EHmRhBDPnJSl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = Generator(100, 128, 3, 768, 256).to(device)","metadata":{"id":"LEccABzlnJSm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator","metadata":{"id":"9WestvffnJSm","outputId":"82ab7e72-0b3c-4ebc-f745-db4d72e930c2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n\n    def __init__(self, num_channels, feature_size, embedding_size, reduced_dim_size):\n        super(Discriminator, self).__init__()\n        self.reduced_dim_size = reduced_dim_size\n\n        self.imageEncoder = nn.Sequential(\n            # 3 -> 128\n            nn.Conv2d(num_channels, feature_size, 4, 2, 1, bias = False),\n            nn.LeakyReLU(0.2, inplace = True),\n\n            # 128 -> 128\n            nn.Conv2d(feature_size, feature_size, 4, 2, 1, bias = False),\n            nn.LeakyReLU(0.2, inplace = True),\n\n            # 128 -> 256\n            nn.Conv2d(feature_size, feature_size * 2, 4, 2, 1, bias = False),\n            nn.BatchNorm2d(feature_size * 2),\n            nn.LeakyReLU(0.2, inplace = True),\n\n            # 256 -> 512\n            nn.Conv2d(feature_size * 2, feature_size * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(feature_size * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # 512 -> 1024\n            nn.Conv2d(feature_size * 4, feature_size * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(feature_size * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n\n        )\n\n        self.textEncoder = nn.Sequential(\n            nn.Linear(in_features=embedding_size, out_features=reduced_dim_size),\n            nn.BatchNorm1d(num_features=reduced_dim_size),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        )\n\n        self.finalBlock = nn.Sequential(\n            nn.Conv2d(feature_size * 8 + reduced_dim_size, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n\n    def forward(self, input_img, text_embeddings):\n        image_encoded = self.imageEncoder(input_img)\n\n        text_encoded = self.textEncoder(text_embeddings)\n\n        replicated_text = text_encoded.repeat(4, 4, 1, 1).permute(2,  3, 0, 1)\n        concat_layer = torch.cat([image_encoded, replicated_text], 1)\n\n        x = self.finalBlock(concat_layer)\n\n        return x.view(-1, 1)","metadata":{"id":"geLaUb6WnJSm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"discriminator = Discriminator(3, 128, 768, 256).to(device)","metadata":{"id":"RIJwAbW_nJSm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"discriminator","metadata":{"id":"3B_F1vETnJSm","outputId":"17fd972e-c79e-4f74-b54f-cf915f26b0c6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bce_loss = nn.BCELoss()","metadata":{"id":"P1Y1LE5anJSm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt_o_text_embeddings = ds[0]['embed_caption'].unsqueeze(0)\nplt_o_text_embeddings.shape","metadata":{"id":"Lio3J9U6nJSm","outputId":"24e2299b-4635-4127-dc16-dd9eba4cf4ac"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fixed_noise = torch.randn(size=(1, 100))\nfixed_noise.shape","metadata":{"id":"Ul69JmkFnJSm","outputId":"e0d5c98c-d0fa-473a-d1ac-4c8430e95c42"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_grid(torchvision.utils.make_grid(ds[0]['image'], normalize=True))","metadata":{"id":"8McF0DzknJSm","outputId":"009c7d33-1527-4fca-f5d0-2d2428fb457c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Training**","metadata":{"id":"C2J9Zt4Rn3IK"}},{"cell_type":"code","source":"def plot_output(generator):\n  plt.clf()\n  with torch.no_grad():\n\n    generator.eval()\n    test_images = generator(fixed_noise.to(device), plt_o_text_embeddings.to(device))\n    generator.train()\n\n    grid = torchvision.utils.make_grid(test_images.cpu(), normalize=True)\n    show_grid(grid)","metadata":{"id":"_LIO3zHGnJSm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\ncriterion = nn.BCELoss()","metadata":{"id":"S4ZuU8nJnJSm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nepochs = 500\n\nfor epoch in range(epochs):\n\n    d_losses, g_losses = [], []\n\n    epoch_time = time.time()\n\n    for batch in dataloader:\n        images = batch['image'].to(device)  # Đưa ảnh lên GPU\n        embed_captions = batch['embed_caption'].to(device)  # Đưa captions lên GPU (đã là tensor)\n\n        real_labels = torch.ones(images.size(0), 1, device=device)\n        fake_labels = torch.zeros(images.size(0), 1, device=device)\n\n        # Training the discriminator\n        optimizer_D.zero_grad()\n\n        noise = torch.randn(size=(images.size(0), 100), device=device)\n        fake_images = generator(noise, embed_captions)\n\n        real_labels = torch.ones(images.size(0), 1, device=device)\n        fake_labels = torch.zeros(images.size(0), 1, device=device)\n\n        real_loss = criterion(\n            discriminator(images, embed_captions),\n            real_labels\n        )\n        fake_loss = criterion(\n            discriminator(fake_images.detach(), embed_captions),\n            fake_labels\n        )\n        d_loss = real_loss + fake_loss\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        d_losses.append(d_loss.item())\n\n        # Training generator\n        optimizer_G.zero_grad()\n        noise = torch.randn(size=(images.size(0), 100), device=device)\n        fake_images = generator(noise, embed_captions)\n\n        g_loss = criterion(\n            discriminator(fake_images, embed_captions),\n            real_labels\n        )\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        g_losses.append(g_loss.item())\n\n    avg_d_loss = sum(d_losses)/len(d_losses)\n    avg_g_loss = sum(g_losses)/len(g_losses)\n\n    if (epoch+1) % 10 == 0:\n        plot_output(generator)\n\n    print('Epoch [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(\n        epoch+1, epochs,\n        avg_d_loss,\n        avg_g_loss,\n        time.time() - epoch_time)\n    )","metadata":{"id":"AUYM1VImnJSm","outputId":"2bc073d8-3745-4da6-d64b-1d9ad05baa7d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Save generator\ntorch.save(generator.state_dict(), 'generator_contrastive.pth')\n\n# Save discriminator\ntorch.save(discriminator.state_dict(), 'discriminator_contrastive.pth')","metadata":{"id":"4fTgRDrsnJSn"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds[4]['text']","metadata":{"id":"ETMS7DspnJSn","outputId":"4ab5ec7f-18a2-479f-9d5c-2f80a857347a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator.eval()\n\nnoise = torch.randn(size=(1, 100))\ntext_embedding = ds[4]['embed_caption'].unsqueeze(0)\nwith torch.no_grad():\n    test_images = generator(noise.to(device), text_embedding.to(device))\ngrid = torchvision.utils.make_grid(test_images.cpu(), normalize=True)\nshow_grid(grid)","metadata":{"id":"DzJv3LNznJSq","outputId":"67b3a3bc-2967-4bc9-b17e-feed7cbdeb80"},"outputs":[],"execution_count":null}]}
